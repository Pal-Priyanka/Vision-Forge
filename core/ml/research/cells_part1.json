[{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\udd2c Transformer vs CNN Object Detection on Pascal VOC 2012\n", "## Comparing YOLOv5 (CNN) and DETR (Vision Transformer)\n", "\n", "---\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\udce6 Section 1: Setup & Data Loading\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1.1 Install & Verify Dependencies\n"]}, {"cell_type": "code", "metadata": {}, "source": ["# Comprehensive dependency installation\n", "# !pip install torch torchvision transformers ultralytics opencv-python pandas matplotlib seaborn tqdm Pillow scipy scikit-learn pycocotools numpy timm\n", "\n", "import importlib\n", "import sys\n", "\n", "def check_dependencies(packages):\n", "    missing = []\n", "    for pkg in packages:\n", "        try:\n", "            importlib.import_module(pkg)\n", "        except ImportError:\n", "            missing.append(pkg)\n", "    \n", "    if missing:\n", "        print(f\"\u274c Missing packages: {', '.join(missing)}\")\n", "        print(\"Please run: !pip install \" + \" \".join(missing))\n", "        print(\"\u26a0\ufe0f IMPORTANT: After installation, you MUST restart your Jupyter kernel (Kernel -> Restart).\")\n", "    else:\n", "        print(\"\u2705 All core dependencies present.\")\n", "\n", "check_dependencies(['torch', 'torchvision', 'transformers', 'ultralytics', 'cv2', 'pandas', 'matplotlib', 'seaborn', 'tqdm', 'PIL', 'scipy', 'sklearn', 'pycocotools', 'numpy', 'timm'])\n"], "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1.2 Import Libraries\n"]}, {"cell_type": "code", "metadata": {}, "source": ["import os\n", "import glob\n", "import json\n", "import time\n", "import random\n", "import logging\n", "import warnings\n", "import xml.etree.ElementTree as ET\n", "from pathlib import Path\n", "from collections import Counter, defaultdict\n", "\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import matplotlib.patches as patches\n", "import seaborn as sns\n", "from PIL import Image\n", "from tqdm.auto import tqdm\n", "from scipy import ndimage\n", "\n", "import torch\n", "import torch.nn as nn\n", "import torch.optim as optim\n", "from torch.utils.data import Dataset, DataLoader\n", "import torchvision\n", "from torchvision import transforms\n", "\n", "# DETR & Transformer specific\n", "import transformers\n", "from transformers import DetrImageProcessor, DetrForObjectDetection\n", "try:\n", "    import timm\n", "    print(f\"\u2705 timm version: {timm.__version__}\")\n", "except ImportError:\n", "    print(\"\u274c timm not found in the current kernel. RESTART YOUR KERNEL NOW.\")\n", "\n", "warnings.filterwarnings('ignore')\n", "logging.basicConfig(level=logging.INFO)\n", "\n", "# Visualization defaults\n", "sns.set_style(\"whitegrid\")\n", "plt.rcParams.update({'figure.figsize': (12, 6), 'figure.dpi': 100})\n", "CB_PALETTE = sns.color_palette(\"colorblind\", 20)\n", "\n", "print(f\"\u2705 PyTorch: {torch.__version__} | Transformers: {transformers.__version__}\")\n"], "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1.3 Device Configuration\n"]}, {"cell_type": "code", "metadata": {}, "source": ["SEED = 42\n", "random.seed(SEED)\n", "np.random.seed(SEED)\n", "torch.manual_seed(SEED)\n", "torch.cuda.manual_seed_all(SEED)\n", "torch.backends.cudnn.deterministic = True\n", "\n", "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n", "print(f\"Using device: {DEVICE}\")\n", "if torch.cuda.is_available():\n", "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"], "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1.4 Define Paths & Constants\n"]}, {"cell_type": "code", "metadata": {}, "source": ["PROJECT_ROOT = Path(r\"C:\\Users\\palan\\OneDrive\\Desktop\\DL_Transformer_project\")\n", "VOC_ROOT = PROJECT_ROOT / \"VOC2012_train_val\" / \"VOC2012_train_val\"\n", "ANNOTATIONS_DIR = VOC_ROOT / \"Annotations\"\n", "IMAGES_DIR = VOC_ROOT / \"JPEGImages\"\n", "IMAGESETS_DIR = VOC_ROOT / \"ImageSets\" / \"Main\"\n", "RESULTS_DIR = PROJECT_ROOT / \"results\"\n", "RESULTS_DIR.mkdir(exist_ok=True)\n", "\n", "VOC_CLASSES = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n", "CLASS_TO_IDX = {cls: i for i, cls in enumerate(VOC_CLASSES)}\n", "\n", "IMG_SIZE_YOLO, IMG_SIZE_DETR = 640, 512\n", "BATCH_SIZE = 8\n", "EPOCHS = 50\n"], "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1.5 Load Annotations\n"]}, {"cell_type": "code", "metadata": {}, "source": ["def parse_voc_xml(xml_path):\n", "    tree = ET.parse(xml_path)\n", "    root = tree.getroot()\n", "    filename = root.find('filename').text\n", "    size = root.find('size')\n", "    img_w, img_h = int(size.find('width').text), int(size.find('height').text)\n", "    \n", "    objects = []\n", "    for obj in root.findall('object'):\n", "        name = obj.find('name').text\n", "        bbox = obj.find('bndbox')\n", "        xmin, ymin = float(bbox.find('xmin').text), float(bbox.find('ymin').text)\n", "        xmax, ymax = float(bbox.find('xmax').text), float(bbox.find('ymax').text)\n", "        objects.append({\n", "            'image_id': Path(xml_path).stem, 'image_path': str(IMAGES_DIR / filename),\n", "            'object_class': name, 'xmin': xmin, 'ymin': ymin, 'xmax': xmax, 'ymax': ymax,\n", "            'bbox_width': xmax - xmin, 'bbox_height': ymax - ymin,\n", "            'image_width': img_w, 'image_height': img_h\n", "        })\n", "    return objects\n", "\n", "train_ids = [l.strip() for l in open(IMAGESETS_DIR / \"train.txt\") if l.strip()]\n", "val_ids = [l.strip() for l in open(IMAGESETS_DIR / \"val.txt\") if l.strip()]\n", "\n", "all_objects = []\n", "# For testing, we only parse first 100 XMLs if env is 'test', otherwise all\n", "xml_files = glob.glob(str(ANNOTATIONS_DIR / \"*.xml\"))\n", "if os.getenv(\"NB_TEST_MODE\") == \"1\":\n", "    xml_files = xml_files[:100]\n", "\n", "for xml_path in tqdm(xml_files, desc=\"Parsing\"):\n", "    all_objects.extend(parse_voc_xml(xml_path))\n", "\n", "df = pd.DataFrame(all_objects)\n", "df['split'] = 'other'\n", "df.loc[df['image_id'].isin(train_ids), 'split'] = 'train'\n", "df.loc[df['image_id'].isin(val_ids), 'split'] = 'val'\n", "print(f\"Total: {len(df)} annots | {df['image_id'].nunique()} images\")\n"], "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\udcca Section 2: EDA (Compact)\n"]}, {"cell_type": "code", "metadata": {}, "source": ["fig, ax = plt.subplots(figsize=(10, 6))\n", "df['object_class'].value_counts().plot(kind='barh', ax=ax, color=CB_PALETTE)\n", "ax.set_title(\"Class Distribution\")\n", "plt.show()\n"], "outputs": [], "execution_count": null}]